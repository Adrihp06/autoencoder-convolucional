{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN67JBjjoRML3+eM8T3NMNX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"Mr-r36OWzkUp","executionInfo":{"status":"error","timestamp":1671044980587,"user_tz":0,"elapsed":5,"user":{"displayName":"Adrián Hernández Padrón","userId":"00474789780436808990"}},"outputId":"8736c84f-df98-4130-d0fe-a99359697547"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-afb2f8ad1b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLearningRateScheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlearning_curve_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, LeakyReLU, Flatten, Lambda, Dense, Activation, Dropout, Reshape, Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger, EarlyStopping\n","from tensorflow.keras.utils import plot_model\n","from utils import learning_curve_plot\n","from tensorflow.keras import backend as K\n","import numpy as np\n","import os\n","import pickle"]},{"cell_type":"code","source":["class ConvAutoencoder:\n","\n","  def __init__(self, \n","               input_dim,\n","               encoder_conv_filters,\n","               encoder_conv_kernels,\n","               encoder_conv_strides,\n","               decoder_conv_filters,\n","               decoder_conv_kernels,\n","               decoder_conv_strides,\n","               z_dim):\n","    self.input_dim = input_dim\n","    self.encoder_conv_filters = encoder_conv_filters\n","    self.encoder_conv_kernels = encoder_conv_kernels\n","    self.encoder_conv_strides = encoder_conv_strides\n","    self.decoder_conv_filters = decoder_conv_filters\n","    self.decoder_conv_kernels = decoder_conv_kernels\n","    self.decoder_conv_strides = decoder_conv_strides\n","    self.z_dim = z_dim\n","\n","    def build(self, use_batch_norm = False, use_dropout = False, VCAE = False):\n","      #Defiminos el encoder\n","      encoder_input = Input(shape = self.input_dim, name = 'encoder_input')\n","      x = encoder_input\n","      for i in range (len(self.encoder_conv_filters)):\n","        conv_layer = Conv2D(filters = self.encoder_conv_filters[i], \n","                            kernel_size = self.encoder_conv_kernels[i],\n","                            strides = self.encoder_conv_strides[i],\n","                            padding = 'same', name = 'encoder_conv'+str(i))\n","        x = conv_layer(x)\n","        if use_batch_norm:\n","          x = BatchNormalization(x)\n","        \n","        x = LeakyReLU(alpha = 0.2)(x)\n","\n","        if use_dropout:\n","          x = Dropout(0.25)(x)\n","        \n","      #Almacenamos la dimensionalidad, obviamo sla primera que indica el tamano del batch\n","      shape_before_flattening = K.int_shape(x)[1:]\n","      x = Flatten()(x)\n","      encoder_output = Dense(self.z_dim, name = 'encoder_output')(x)\n","\n","      #La versión variacional pretende regularizar el espacio latente. a partir de distribuciones de probabilidad multivariante\n","      if VCAE:\n","        #Se define por un vector de medias y un vector de desviaciones\n","        self.mu = Dense(self.z_dim, name = 'mu')(x)\n","        self.log_var = Dense(self.z_dim, name = 'log_var')(x)\n","        #Tenemos que definir al funcion sampling\n","        def sampling(args):\n","          mu, log_var = args\n","          #Definimos la ecuación de la gaussiana\n","          epsilon = K.random_normal(shape = K.shape(mu), mean = 0., stddev=1.)\n","          return mu + K.exp(log_var/2)*epsilon\n","        encoder_output = Lambda(sampling, name ='encoder_output')([self.mu, self.log_vaer]) \n","\n","\n","      self.encoder = Model(encoder_input, encoder_output)\n","      #Definimos el decoder\n","      decoder_input = Input(shape=(self.z_dim,), name = 'decoder_input')\n","      #Tenemos que pasar de la dimensionalidad del espacio latente a la dimensionalidad de antes del flattening\n","      x = Dense(np.prod(shape_before_flattening))(decoder_input)\n","      x = Reshape(shape_before_flattening)(x)\n","\n","      for i in range (len(self.decoder_conv_filters)):\n","        conv_layer_t = Conv2DTranspose(filters = self.decoder_conv_filters[i], \n","                                     kernel_size = self.decoder_conv_kernels[i],\n","                                     strides = self.decoder_conv_strides[i],\n","                                     padding = 'same', name = 'decoder_conv'+str(i))\n","        \n","        x = conv_layer_t(x)\n","\n","        if i < len(self.decoder_conv_filters) - 1:\n","          x = LeakyReLU(alpha = 0.2)(x)\n","        else:\n","          #Si estamos en la ultima capa devolvemos a valores entre 0 y 1\n","          x = Activation('sigmoid')(x)\n","\n","      decoder_output = x\n","      self.decoder = Model(decoder_input, decoder_output)\n","\n","      #Creando el autoencoder convolucional\n","\n","      autoencoder_input = encoder_input\n","      autoencoder_output = self.decoder(encoder_output)\n","      autoencoder = Model(autoencoder_input, autoencoder_output)\n","      self.model = autoencoder\n","\n","  def compile(self, learning_rate=0.005, r_loss_factor = 0.4, VCAE = False):\n","    self.learning_rate = learning_rate\n","    self.r_loss_factor = r_loss_factor\n","    optimazer = Adam(lr = learning_rate)\n","    #Definimos las pérdidas en caso del variacional y para ello tenemos que determinar una pérdida asociada al mse y otra al espacio latente\n","    #Debido a esto ponderamos la pérdida asociado al espacio latente como un hiperparámetro mas\n","    if VCAE:\n","      def vae_r_loss(y_true, y_pred):\n","        r_loss = K.mean(K.square(y_true-y_pred)**2, axis = [1,2,3])\n","        return r_loss * r_loss_factor\n","      def vae_kl_loss(y_true, y_pred):\n","        kl_loss = -0.5 * K.sum(1 + self.log_var - K.square(self.mu) - K.exp(self.log_var, axis = 1))\n","        return kl_loss\n","      def vae_loss(y_true, y_pred):\n","        r_loss = vae_r_loss(y_true, y_pred)\n","        kl_loss = vae_kl_loss(y_true, y_pred)\n","        return r_loss + kl_loss\n","    else:\n","      self.model.compile(optimazer = optimazer, loss = 'mse')\n","\n","  def train(self, data_flow, epochs, steps_per_epoch, data_flow_val, run_folders):\n","    csv_logger = CSVLogger(run_fodlers[\"log_filename\"])\n","    checkpoint = ModelCheckPoint(os.path.join(run_folders[\"model_path\"], run_folders[\"exp_name\"]+'weights/CAE_weights.h5', save_wigths_only = True, verbose = 1))\n","    lr_sched = self.step_decay_schedule(initial_lr = self.learning_rate, decay_factor = 0.1, step_size = 1)\n","    early_stop = EarlyStopping(monitor = 'val_loss', patience = 25)\n","    callback_list = [csv_logger, checkpoint, lr_sched, early_stop]\n","    print(\"[INFO]: Training\")\n","    history = self.model.fit(data_flow, \n","                             epochs=epochs, \n","                             validation_data=data_flow_val, \n","                             steps_per_epoch = steps_per_epoch, \n","                             callbacks = callbacks_list)\n","    self.save_model(run_folders, history)\n","\n","#Reescribimos el metodo step_decay_schedule de Keras para poder aplicar nuestros parametros.\n","  def step_decay_schedule(initial_lr , decay_factor = 0.5, step_size = 1):\n","    def schedule(epoch):\n","      new_lr = initial_lr * (decay_factor ** np.floor(epoch/step_size))\n","      return new_lr\n","    return LearningRateScheduler(schedule)\n","\n","#Metodos de carga y almacenamiento del modelo\n","  def save_model(self, run_folders, history):\n","    with open(os.path.join(run_folders[\"model_path\"],  run_folders[\"exp_name\"] + '/CAE_model.pkl'), 'wr') as f:\n","    pickle.dump([self.input_dim = input_dim\n","                , self.encoder_conv_filters \n","                , self.encoder_conv_kernels\n","                , self.encoder_conv_strides \n","                , self.decoder_conv_filters \n","                , self.decoder_conv_kernels\n","                , self.decoder_conv_strides \n","                , self.z_dim ], f)\n","    self.plot_model(run_folders)\n","    learning_curve_plot(history, run_folders)\n","\n","  @staticmethod\n","  def load_model(run_folders):\n","    with open(os.path.join(run_folders[\"model_path\"],  run_folders[\"exp_name\"] + '/CAE_model.pkl'), 'rb') as f:\n","      params = pickle.load(f)\n","\n","    my_CAE = ConvAutoencoder(*params)\n","    my_CAE.build(use_batch_norm = True, use_dropout = False, VCAE = False)\n","    my_CAE.model.load_weights(os.path.join(run_folders[\"model_path\"], run_folders[\"exp_name\"]+'weights/CAE_weights.h5'))\n","    return my_CAE\n","#Almacener una imagen con la arquitectura de la red\n","    def plot_model(self, run_folders):\n","      plot_model(self.model, to_file = os.path.join(run_folders[\"model_path\"], run_folders[\"exp_name\"]+'/viz/model_autoencoder.png'), show_shapes = True, show_layers_names)\n","      plot_model(self.encoder, to_file = os.path.join(run_folders[\"model_path\"], run_folders[\"exp_name\"]+'/viz/model_encoder.png'), show_shapes = True, show_layers_names)\n","      plot_model(self.decoder, to_file = os.path.join(run_folders[\"model_path\"], run_folders[\"exp_name\"]+'/viz/model_decoder.png'), show_shapes = True, show_layers_names)"],"metadata":{"id":"6d9_HcnW0nr9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FVAL0EA-8rZK"},"execution_count":null,"outputs":[]}]}